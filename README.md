# Healthcare Provider Fraud Detection (Medicare Claims)

Endâ€‘toâ€‘end machineâ€‘learning pipeline to flag **potentially fraudulent healthcare providers** from Medicare claim patterns. The repository is **codeâ€‘first (Python)** and all **CSVs in this repo are programâ€‘generated outputs** (processed datasets and submissions) â€” not the original raw Kaggle files.

---

## ğŸš€ TL;DR
- **Goal:** Predict `PotentialFraud` at the **provider** level using inpatient/outpatient/beneficiary signals.
- **Approach:** Clean & merge â†’ feature engineering â†’ classâ€‘imbalance handling â†’ train multiple models â†’ choose operating threshold â†’ persist artifacts and outputs.
- **Artifacts:** Trained models (`.joblib`), metadata (`.json`), and **CSV outputs saved by the script** for reproducibility.

---

## ğŸ“Š Dataset
- **Source:** Kaggle â€” [Healthcare Provider Fraud Detection Analysis](https://www.kaggle.com/datasets/rohitrox/healthcare-provider-fraud-detection-analysis/data)
- **Kaggle link (provided):** https://www.kaggle.com/datasets/rohitrox/healthcare-provider-fraud-detection-analysis/data
- **Tables:** Beneficiary, Inpatient, Outpatient, and Provider label table.
- **Target:** `PotentialFraud` (Yes/No) at provider level.

> ğŸ” **Note:** Raw Kaggle CSVs are **not** committed. The repository contains **processed CSVs** and **submission CSVs** that are **saved automatically by the program** during runs.

---

## ğŸ—‚ï¸ Whatâ€™s in this Repo
Actual file names may vary across runs, but you will typically see:

```
â”œâ”€ healthcare_fraud_detection.py     # Main pipeline script (code-first project)
â”œâ”€ models/                           # Trained models + transformers + metadata
â”‚  â”œâ”€ logistic_regression_threshold_60.joblib
â”‚  â”œâ”€ random_forest.joblib
â”‚  â”œâ”€ isolation_forest.joblib
â”‚  â”œâ”€ isolation_forest_pca_scaler.joblib
â”‚  â”œâ”€ isolation_forest_pca_transformer.joblib
â”‚  â”œâ”€ logistic_regression_scaler.joblib
â”‚  â”œâ”€ *metadata.json                 # feature list, thresholds, versions
â”‚  â””â”€ ...
â”œâ”€ data/
â”‚  â”œâ”€ processed/                     # Program-saved processed datasets (features, joins)
â”‚  â””â”€ submissions/                   # Program-saved prediction CSVs (see below)
â”‚     â”œâ”€ Submission_logistic_regression_threshold_60.csv
â”‚     â”œâ”€ Submission_Random_Forest_Classifier.csv
â”‚     â””â”€ Submission_Isolation_Forest.csv
â”œâ”€ README.md
â””â”€ requirements.txt
```

> If you see these files at the **repo root** currently, thatâ€™s fine functionally. For a more professional layout, consider placing them under the `models/` and `data/` folders as shown above.

---

## âš™ï¸ Setup
```bash
# 1) Create & activate a virtual environment
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
# source .venv/bin/activate

# 2) Install dependencies
pip install -r requirements.txt
```
**Suggested requirements (edit if your versions differ):**
```
pandas>=2.0
numpy>=1.24
scikit-learn>=1.3
imbalanced-learn>=0.11
matplotlib>=3.7
joblib>=1.3
```

---

## ğŸ§ª How to Run the Pipeline
> The project is implemented as a single Python program. Paths and options are defined in the script; some runs may expose CLI flags, but the default is a oneâ€‘command run.

### Option A â€” One command
```bash
python healthcare_fraud_detection.py
```
This will:
- Load/prepare data (from your configured paths)
- Engineer providerâ€‘level features
- Train models (Logistic Regression, Random Forest, Isolation Forest)
- Apply thresholding (e.g., 0.60 for the LR variant in this run)
- **Save processed datasets and predictions as CSVs** under `data/processed/` and `data/submissions/`
- Persist trained models + scalers + metadata to `models/`

### Option B â€” With explicit arguments (if enabled)
```bash
python healthcare_fraud_detection.py \
  --raw_dir data/raw \
  --processed_dir data/processed \
  --models_dir models \
  --submissions_dir data/submissions \
  --model lr --threshold 0.60 --seed 42
```
> If the above flags are not implemented in your local copy, use **Option A** and edit the configuration constants at the top of the script.

---

## ğŸ“¦ Outputs (Saved by the Program)
- **Models & Transformers** (`models/`)
  - `logistic_regression_threshold_60.joblib`
  - `random_forest.joblib`
  - `isolation_forest.joblib`
  - `logistic_regression_scaler.joblib`
  - `isolation_forest_pca_scaler.joblib`, `isolation_forest_pca_transformer.joblib`
  - `*_metadata.json` (feature order, preprocessing config, thresholds, versions)
- **Programâ€‘Generated CSVs**
  - `data/processed/*.csv` â€” cleaned/merged/engineered datasets at provider level
  - `data/submissions/Submission_*.csv` â€” predictions for evaluation/submit

> âœ… These CSVs are included in the repo intentionally to make the run **reproducible** for reviewers without reâ€‘processing the raw Kaggle files.

---

## ğŸ—ï¸ Methodology (Highâ€‘Level)
1. **Cleaning & Integration** â€” Join Beneficiary, Inpatient, Outpatient claims â†’ providerâ€‘level table.
2. **Feature Engineering** â€” Claim counts, unique beneficiaries, reimbursement and deductible stats, LOS metrics, procedure/diagnosis diversity, temporal patterns.
3. **Imbalance Handling** â€” Class weights and/or sampling where appropriate.
4. **Modeling** â€”
   - **Logistic Regression** (with scaler)
   - **Random Forest Classifier**
   - **Isolation Forest** (unsupervised anomaly signals, optionally with PCA)
5. **Evaluation** â€” Accuracy, Precision, Recall, F1, ROCâ€‘AUC; select threshold to favor recall at acceptable precision.
6. **Persistence** â€” Save models, scalers, metadata; export processed datasets and submissions as CSVs.

---

## ğŸ”® Inference Example (Load a Saved Model)
```python
import joblib
import pandas as pd

# Provider-level features must match the training feature list & order
X = pd.read_csv("data/processed/provider_features.csv")
clf = joblib.load("models/logistic_regression_threshold_60.joblib")
proba = clf.predict_proba(X)[:, 1]
X.assign(fraud_risk=proba).to_csv("data/submissions/predictions_lr.csv", index=False)
```
> Always keep the **feature order** consistent with the order stored in your `*_metadata.json`.

---

## ğŸ” Reproducibility & Notes
- Fixed random seed where applicable (default `42`).
- `requirements.txt` captures core dependencies; pin exact versions for strict reproducibility.
- Large artifacts (>100 MB) should use **Git LFS** or external storage.

---

## ğŸ§­ Responsible Use
This project flags **potential** fraud for investigation. It **does not** determine guilt. Any deployment should include human oversight, audit logs, bias checks, and policyâ€‘aligned thresholds.

---

## ğŸ™Œ Acknowledgments
- Kaggle dataset: *Healthcare Provider Fraud Detection Analysis*.
- Python ecosystem: pandas, scikitâ€‘learn, numpy, imbalancedâ€‘learn, matplotlib.

---

## ğŸ—ºï¸ Roadmap (Optional)
- [ ] Move files into `models/` and `data/â€¦` folders for a cleaner structure
- [ ] Add a tiny `demo.ipynb` (cleared outputs) for quick walkthrough
- [ ] Add CLI flags/config file for reproducible experiments
- [ ] Add unit tests for data prep & feature builders
