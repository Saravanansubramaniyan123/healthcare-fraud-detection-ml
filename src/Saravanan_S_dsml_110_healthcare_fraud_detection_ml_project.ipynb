{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a62758-c20b-40b0-b558-9d2ca76d7e9a",
   "metadata": {},
   "source": [
    "# Healthcare Fraud Detection - Machine Learning Project\n",
    "\n",
    "**Author:** Saravanan S  \n",
    "**Date:** September 2025  \n",
    "**Python Version:** 3.12  \n",
    "**Libraries:** scikit-learn 1.6.1, pandas 2.2.3, numpy 1.26.4\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Project Overview\n",
    "\n",
    "This project implements multiple machine learning approaches to detect fraudulent healthcare providers using Medicare claims data. The solution combines supervised and unsupervised learning techniques to identify suspicious billing patterns and potential fraud cases.\n",
    "\n",
    "**ðŸ† Key Achievement:** Built three distinct models achieving **87-92% accuracy** in fraud detection with comprehensive feature engineering from healthcare claims data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09cc03-cd1c-4bc9-9001-5a3a7dab96bc",
   "metadata": {},
   "source": [
    "## ðŸ“Š Executive Summary\n",
    "\n",
    "### ðŸ” Business Problem\n",
    "Healthcare fraud costs billions annually. This project identifies fraudulent healthcare providers by analyzing:\n",
    "- Patient demographics and chronic health conditions\n",
    "- Claims patterns and billing amounts  \n",
    "- Provider behavior and physician relationships\n",
    "- Treatment duration and medical procedures\n",
    "\n",
    "### ðŸ›  Solution Approach\n",
    "- **Data Integration:** Combined 4 datasets (Claims, Beneficiary, Inpatient, Outpatient)\n",
    "- **Feature Engineering:** Created **156+ engineered features** from 57 original columns\n",
    "- **Model Development:** Implemented 3 distinct approaches with different strengths\n",
    "\n",
    "### ðŸ“ˆ Key Results Summary\n",
    "\n",
    "| Model | Accuracy | F1-Score | Recall | Precision | Best For |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **Logistic Regression** | **92.3%** | 62.4% | 68.4% | 57.5% | Interpretability |\n",
    "| **Random Forest** | 87.2% | 55.9% | **86.8%** | 42.0% | Fraud Detection |\n",
    "| **Isolation Forest** | 92.0% | 54.0% | 53.0% | 55.0% | Anomaly Detection |\n",
    "\n",
    "**ðŸŽ¯ Production Recommendation:** Logistic Regression for balanced performance and interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad8d13-a62e-4bdd-bbfc-6213bda3ee22",
   "metadata": {},
   "source": [
    "## ðŸ“Š Dataset Analysis Summary\n",
    "\n",
    "### ðŸ“ Data Sources\n",
    "| Dataset | Training Shape | Test Shape | Key Information |\n",
    "|---------|----------------|------------|-----------------|\n",
    "| **Provider Labels** | (5,410, 2) | (1,353, 1) | Fraud/No-Fraud labels |\n",
    "| **Beneficiary Data** | (138,556, 25) | (63,968, 25) | Patient demographics & conditions |\n",
    "| **Inpatient Claims** | (40,474, 30) | (9,551, 30) | Hospital admission records |\n",
    "| **Outpatient Claims** | (517,737, 27) | (125,841, 27) | Clinic visit records |\n",
    "\n",
    "### ðŸ”‘ Key Data Insights Discovered\n",
    "- **Class Imbalance:** Only **9.35%** of providers are fraudulent in training data\n",
    "- **Geographic Distribution:** State 5 has highest beneficiary concentration (9.2%)\n",
    "- **Demographics:** 84.5% of beneficiaries are Race category 1\n",
    "- **Claims Volume:** Final aggregated dataset: **558,211 claims** for training\n",
    "- **Feature Expansion:** **156 features** created from original 57 through advanced aggregations\n",
    "\n",
    "### ðŸŽ¨ Data Processing Pipeline\n",
    "1. **Data Cleaning:** Converted dates, handled missing values, standardized categorical variables\n",
    "2. **Feature Engineering:** Provider-level aggregations, beneficiary patterns, physician interactions\n",
    "3. **Data Merging:** Combined all datasets on Provider and BeneID keys\n",
    "4. **Scaling:** StandardScaler normalization for all numerical features\n",
    "5. **PCA:** Dimensionality reduction to 29 components for Isolation Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943599d-e800-4936-863d-cb08ef18d341",
   "metadata": {},
   "source": [
    "### ðŸ›  Technical Implementation Details\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Feature Engineering Highlights\n",
    "**Advanced Feature Engineering Summary**\n",
    "* **Provider Aggregations:** Features like `PerProviderAvg_InscClaimAmtReimbursed`, `PerProviderAvg_DeductibleAmtPaid`, and `PerProviderAvg_Age` were created to summarize claims data at the provider level.\n",
    "* **Beneficiary Patterns:** Features such as `PerBeneIDAvg_InscClaimAmtReimbursed` and `PerBeneIDAvg_Admit_For_Days` were used to capture patterns related to individual beneficiaries.\n",
    "* **Physician Relationships:** Metrics like `PerAttendingPhysicianAvg_InscClaimAmtReimbursed` and `PerOperatingPhysicianAvg_InscClaimAmtReimbursed` were engineered to analyze financial patterns associated with physicians.\n",
    "* **Claims Counting:** `ClmCount_Provider_BeneID` and `ClmCount_Provider_AttendingPhysician` were used to count claims for specific provider-beneficiary and provider-physician pairs.\n",
    "\n",
    "**Total:** 156 engineered features were created for the models.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‹ Model Configurations\n",
    "| Model | Parameter | Value |\n",
    "| :--- | :--- | :--- |\n",
    "| **Logistic Regression** | `algorithm` | `LogisticRegressionCV` |\n",
    "| | `cross_validation` | 10 |\n",
    "| | `class_weight` | `balanced` |\n",
    "| | `threshold` | 60.0% |\n",
    "| | `features` | 156 |\n",
    "| **Random Forest** | `algorithm` | `RandomForestClassifier` |\n",
    "| | `n_estimators` | 500 |\n",
    "| | `max_depth` | 4 |\n",
    "| | `class_weight` | `balanced` |\n",
    "| | `features` | 156 |\n",
    "| **Isolation Forest** | `algorithm` | `IsolationForest` |\n",
    "| | `contamination` | 9.0% |\n",
    "| | `n_estimators` | 100 |\n",
    "| | `pca_components` | 29 |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Model Training Strategy\n",
    "* **Train/Validation Split:** The dataset was split using a **70%/30%** stratified approach.\n",
    "* **Evaluation Metrics:** The models were evaluated using **Accuracy, Precision, Recall, F1-Score, and ROC-AUC**.\n",
    "* **Threshold Tuning:** The classification threshold for the Logistic Regression model was optimized at **60.0%**.\n",
    "* **Cross-Validation:** **10-fold cross-validation** was used for hyperparameter selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b27655-ea21-4fd9-9e54-7bf06257b7d9",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Detailed Model Performance Analysis\n",
    "\n",
    "### ðŸ“Š Complete Performance Metrics\n",
    "### Detailed Results Summary\n",
    "\n",
    "### Detailed Results Summary\n",
    "\n",
    "| Model | Metric | Value |\n",
    "| :--- | :--- | :--- |\n",
    "| **Logistic Regression (Threshold=0.60)** | Train Accuracy | 92.4% |\n",
    "| | Validation Accuracy | 92.3% |\n",
    "| | Train F1 | 59.0% |\n",
    "| | Validation F1 | 62.5% |\n",
    "| | ROC-AUC | 81.6% |\n",
    "| | Test Predictions | 131 fraud cases (9.7%) |\n",
    "| **Random Forest** | Train Accuracy | 87.5% |\n",
    "| | Validation Accuracy | 87.2% |\n",
    "| | Train F1 | 57.4% |\n",
    "| | Validation F1 | 55.9% |\n",
    "| | ROC-AUC | 87.0% |\n",
    "| | Top Feature | PerOperatingPhysicianAvg_InscClaimAmtReimbursed |\n",
    "| **Isolation Forest (Unsupervised)** | Train Accuracy | 92.0% |\n",
    "| | Precision | 55.0% |\n",
    "| | Recall | 53.0% |\n",
    "| | PCA Variance Explained | 96.4% (29 components) |\n",
    "| | Test Predictions | 114 fraud cases (8.4%) |\n",
    "\n",
    "### ðŸ† Model Selection Rationale\n",
    "- **Best Overall:** Logistic Regression (92.3% accuracy + interpretable)\n",
    "- **Best Fraud Detection:** Random Forest (86.8% recall)\n",
    "- **Best Anomaly Detection:** Isolation Forest (unsupervised approach)\n",
    "\n",
    "### ðŸ’¾ Production Assets Generated\n",
    "âœ… **All models serialized** with joblib  \n",
    "âœ… **Scalers preserved** for preprocessing  \n",
    "âœ… **Metadata tracked** in JSON format  \n",
    "âœ… **Test submissions** generated for evaluation  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22013a80-c26e-4ba4-9453-2ac0a86a1189",
   "metadata": {},
   "source": [
    "## ðŸ“ Project Structure & Generated Files\n",
    "\n",
    "### ðŸ¤– Saved Models (Production Ready)\n",
    "\n",
    "| File | Description |\n",
    "| :--- | :--- |\n",
    "| `logistic_regression_baseline.joblib` | The final trained Logistic Regression model. |\n",
    "| `logistic_regression_scaler.joblib` | The fitted scaler for the Logistic Regression model. |\n",
    "| `random_forest_baseline.joblib` | The final trained Random Forest model. |\n",
    "| `random_forest_scaler.joblib` | The fitted scaler for the Random Forest model. |\n",
    "| `isolation_forest_baseline.joblib` | The final trained Isolation Forest model. |\n",
    "| `isolation_forest_pca_scaler.joblib` | The fitted scaler for data before PCA transformation. |\n",
    "| `isolation_forest_pca_transformer.joblib` | The fitted PCA transformer for dimensionality reduction. |\n",
    "\n",
    "### ðŸ“‹ Test Submissions\n",
    "- `Submission_logistic_regression_threshold_60.csv` (1,353 predictions)\n",
    "- `Submission_Random_Forest_Classifier.csv` (1,353 predictions) \n",
    "- `Submission_Isolation_Forest.csv` (1,353 predictions)\n",
    "\n",
    "\n",
    "### ðŸ“ˆ Visualizations Created\n",
    "- Provider fraud distribution analysis\n",
    "- ROC curves and precision-recall curves\n",
    "- Feature importance rankings\n",
    "- Geographic fraud patterns by state\n",
    "- Top diagnosis codes in fraudulent cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae414c-c5d5-4d95-9bb3-f8a551043946",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Key Findings & Recommendations\n",
    "\n",
    "### ðŸ’¡ Major Discoveries\n",
    "1. **Feature engineering is crucial** - 156 features from 57 original columns significantly improved performance\n",
    "2. **Provider aggregations most predictive** - Average claim amounts per provider strongest fraud indicators  \n",
    "3. **Multi-model approach valuable** - Different models excel in different aspects (accuracy vs recall)\n",
    "4. **Unsupervised methods competitive** - Isolation Forest achieved 92% accuracy without labels\n",
    "\n",
    "### ðŸš€ Production Deployment Recommendations\n",
    "- **Primary Model:** Logistic Regression (best balance of accuracy + interpretability)\n",
    "- **High-Recall Scenario:** Random Forest (catches 86.8% of fraud cases)\n",
    "- **Real-time Scoring:** Ensemble combining all three models\n",
    "- **Threshold Tuning:** Adjust based on business cost of false positives/negatives\n",
    "\n",
    "### ðŸ”® Future Enhancements\n",
    "- [ ] **Temporal Analysis:** Time-series patterns in fraudulent behavior\n",
    "- [ ] **Network Analysis:** Provider-patient relationship graphs  \n",
    "- [ ] **Deep Learning:** Neural networks for complex pattern detection\n",
    "- [ ] **Real-time Pipeline:** Streaming fraud detection system\n",
    "- [ ] **Cost-Benefit Analysis:** Optimize thresholds based on financial impact\n",
    "- [ ] **External Data:** Integrate pharmacy, lab, and claims databases\n",
    "\n",
    "### ðŸ“š Lessons Learned\n",
    "- Healthcare data requires extensive domain knowledge and careful feature engineering\n",
    "- Class imbalance handling critical (used balanced class weights)\n",
    "- Model interpretability essential for healthcare compliance and trust\n",
    "- Comprehensive evaluation beyond accuracy necessary for fraud detection\n",
    "- Proper model serialization crucial for production deployment\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ¯ This project demonstrates end-to-end machine learning pipeline for healthcare fraud detection with production-ready models and comprehensive evaluation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f5972-d986-4867-8515-01ddb8b56720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Visualization setup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility - CRITICAL in production\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Project constants\n",
    "LABELS = [\"Normal\", \"Fraud\"]\n",
    "\n",
    "# Environment info for debugging\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Scikit-learn: {sklearn.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b48f3b1-25cd-42e6-8ea8-aa5d57578d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train dataset\n",
    "train = pd.read_csv(\"Train-1542865627584.csv\")\n",
    "train_ben_data = pd.read_csv(\"Train_Beneficiarydata-1542865627584.csv\")\n",
    "train_inptn_data = pd.read_csv(\"Train_Inpatientdata-1542865627584.csv\")\n",
    "train_outptn_data = pd.read_csv(\"Train_Outpatientdata-1542865627584.csv\")\n",
    "\n",
    "#loading test data\n",
    "test = pd.read_csv(\"Test-1542969243754.csv\")\n",
    "test_ben_data = pd.read_csv(\"Test_Beneficiarydata-1542969243754.csv\")\n",
    "test_inptn_data = pd.read_csv(\"Test_Inpatientdata-1542969243754.csv\")\n",
    "test_outptn_data = pd.read_csv(\"Test_Outpatientdata-1542969243754.csv\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7a69d-7c5b-4718-83b9-8c38803c909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train dataset shape:\",train.shape)\n",
    "print(\"train benificiary data shape:\",train_ben_data.shape)\n",
    "print(\"train inpatient data shape:\",train_inptn_data.shape)\n",
    "print(\"train outpatient data shape:\",train_outptn_data.shape)\n",
    "\n",
    "print(\"test dataset shape:\",test.shape)\n",
    "print(\"test benificiary data shape:\",test_ben_data.shape)\n",
    "print(\"test inpatient data shape:\",test_inptn_data.shape)\n",
    "print(\"test outpatient data shape:\",test_outptn_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bfcdfa-9bda-4a32-ace2-e075dc0c282c",
   "metadata": {},
   "source": [
    "**train and test dataset** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb8290-20af-4608-8753-7c35eac5f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check train and test dataset\n",
    "print(\"train data:\\n\")\n",
    "print(train.head(5))\n",
    "print(\"test data:\\n\")\n",
    "print(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e18036-3024-4068-a838-cf9493994250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check provider details is unique or not\n",
    "print(train[\"Provider\"].value_counts().head(5))\n",
    "print(train[\"Provider\"].value_counts().shape)\n",
    "\n",
    "#checking a null values\n",
    "print(\"No.of.null values in train data:\",train.isnull().sum().sum())\n",
    "print(\"No.of.null values in test data:\",test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dea028-0f44-43ee-ad93-d53ac9a26fef",
   "metadata": {},
   "source": [
    "**train and test beneficiary dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303e64f-4c27-45d9-801c-b2ccd7df119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.columns\",None)\n",
    "train_ben_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecfd23-6911-4888-9892-7b84dc851bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ben_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6dfc5-81e7-4b17-9acf-394e32269eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_beneficiary dataset datatypes:\\n\",train_ben_data.dtypes)\n",
    "print(\"--\"*50)\n",
    "print(\"test_beneficiary dataset datatypes:\\n\",test_ben_data.dtypes)\n",
    "#checking a null values\n",
    "print(\"Null values in train_ben_data:\",train_ben_data.isnull().sum())\n",
    "print(\"Null values in test_ben_data:\",test_ben_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323a660-8a75-4a13-8652-280ce07d3e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all the disease contains 1->yes and 2->No so we have to convert 2 into 0.\n",
    "train_ben_data = train_ben_data.replace({\"ChronicCond_Alzheimer\":2,\n",
    "\"ChronicCond_Heartfailure\":2,\n",
    "\"ChronicCond_KidneyDisease\":2,\n",
    "\"ChronicCond_Cancer\":2,\n",
    "\"ChronicCond_ObstrPulmonary\":2,\n",
    "\"ChronicCond_Depression\":2,\n",
    "\"ChronicCond_Diabetes\":2,\n",
    "\"ChronicCond_IschemicHeart\":2,\n",
    "\"ChronicCond_Osteoporasis\":2,\n",
    "\"ChronicCond_rheumatoidarthritis\":2,\n",
    "\"ChronicCond_stroke\":2},0)\n",
    "\n",
    "test_ben_data = test_ben_data.replace({\"ChronicCond_Alzheimer\":2,\n",
    "\"ChronicCond_Heartfailure\":2,\n",
    "\"ChronicCond_KidneyDisease\":2,\n",
    "\"ChronicCond_Cancer\":2,\n",
    "\"ChronicCond_ObstrPulmonary\":2,\n",
    "\"ChronicCond_Depression\":2,\n",
    "\"ChronicCond_Diabetes\":2,\n",
    "\"ChronicCond_IschemicHeart\":2,\n",
    "\"ChronicCond_Osteoporasis\":2,\n",
    "\"ChronicCond_rheumatoidarthritis\":2,\n",
    "\"ChronicCond_stroke\":2},0)\n",
    "\n",
    "train_ben_data = train_ben_data.replace({\"RenalDiseaseIndicator\":\"Y\"},1)\n",
    "test_ben_data = test_ben_data.replace({\"RenalDiseaseIndicator\":\"Y\"},1)\n",
    "test_ben_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23746f-601d-4895-ac6c-98612bedaa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting DOB and DOD into datatimetype based on that calculate the age for the person.\n",
    "train_ben_data[\"DOB\"] = pd.to_datetime(train_ben_data[\"DOB\"])\n",
    "train_ben_data[\"DOD\"] = pd.to_datetime(train_ben_data[\"DOD\"],errors = \"ignore\")\n",
    "test_ben_data[\"DOB\"] = pd.to_datetime(test_ben_data[\"DOB\"])\n",
    "test_ben_data[\"DOD\"] = pd.to_datetime(test_ben_data[\"DOD\"],errors = \"ignore\")\n",
    "print(train_ben_data.dtypes)\n",
    "test_ben_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba25a72-e74e-43f5-a2fb-d651ded4f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new column age and add it into a test and train_ben_dataset\n",
    "train_ben_data[\"Age\"] = round(((train_ben_data[\"DOD\"] - train_ben_data[\"DOB\"]).dt.days)/365)\n",
    "test_ben_data[\"Age\"] = round(((test_ben_data[\"DOD\"] - test_ben_data[\"DOB\"]).dt.days)/365)\n",
    "print(train_ben_data.head(10))\n",
    "test_ben_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e80e0-840f-4af9-a273-676e1591628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"we can't able to calculate age for a persons who has a dod,so because of the we take last dod that registered in\n",
    "the dataset and based on that we calculate the age for other persons\"\"\"\n",
    "last_death = (train_ben_data[\"DOD\"].max())\n",
    "train_ben_data[\"Age\"] = train_ben_data[\"Age\"].fillna(round(((last_death - train_ben_data[\"DOB\"]).dt.days)/365))\n",
    "test_ben_data[\"Age\"] = test_ben_data[\"Age\"].fillna(round(((last_death - test_ben_data[\"DOB\"]).dt.days)/365))\n",
    "print(train_ben_data.head(10))\n",
    "test_ben_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f28bb1-26c9-4eaf-aed0-18b48f402e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column that shows the patient dead or not.\n",
    "# using boolean indexing we are going to acheive this,if the DOD is not null the person is not dead \"0\" else \"1\".\n",
    "train_ben_data.loc[train_ben_data[\"DOD\"].isnull(),\"WhetherDead\"] = 0\n",
    "train_ben_data.loc[train_ben_data[\"DOD\"].notnull(),\"WhetherDead\"] = 1\n",
    "print(train_ben_data.head(2))\n",
    "\n",
    "test_ben_data.loc[test_ben_data[\"DOD\"].isnull(),\"WhetherDead\"] = 0\n",
    "test_ben_data.loc[test_ben_data[\"DOD\"].notnull(),\"WhetherDead\"] = 1\n",
    "test_ben_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bd6ac-04fc-4f6d-83a5-046e1390520d",
   "metadata": {},
   "source": [
    "### **Inpatient dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f0a15-ffcc-413d-b3aa-7380fa352a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inptn_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320f22d-4714-407e-9472-c9603b4156c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inptn_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752d4c3-69c5-42ef-ae6f-6e1843ffb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values in inpatient dataset\n",
    "print(\"train inpatient data null values:\\n\",train_inptn_data.isnull().sum())\n",
    "print(train_inptn_data.shape)\n",
    "print(\"---\"*100)\n",
    "print(\"test inpatient data null values:\\n\",test_inptn_data.isnull().sum())\n",
    "print(test_inptn_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0b055-a537-436a-9893-a1ff1daa552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting a admission date and discharge date column into datetime type\n",
    "train_inptn_data[\"AdmissionDt\"] = pd.to_datetime(train_inptn_data[\"AdmissionDt\"])\n",
    "train_inptn_data[\"DischargeDt\"] = pd.to_datetime(train_inptn_data[\"DischargeDt\"])\n",
    "test_inptn_data[\"AdmissionDt\"] = pd.to_datetime(test_inptn_data[\"AdmissionDt\"])\n",
    "test_inptn_data[\"DischargeDt\"] = pd.to_datetime(test_inptn_data[\"DischargeDt\"])\n",
    "print(train_inptn_data.dtypes)\n",
    "print(test_inptn_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b43b7e-de27-4d84-a932-0b15fc46a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column Admit_For_Days\n",
    "#we are adding 1 day extra day for each because if the patient admit and discharge in same day it should be considered as one not \"0\".\n",
    "train_inptn_data[\"Admit_For_Days\"] = ((train_inptn_data[\"DischargeDt\"] - train_inptn_data[\"AdmissionDt\"]).dt.days) + 1\n",
    "test_inptn_data[\"Admit_For_Days\"] = ((test_inptn_data[\"DischargeDt\"] - test_inptn_data[\"AdmissionDt\"]).dt.days) + 1\n",
    "print(train_inptn_data.head(2))\n",
    "test_inptn_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a4330-8860-477e-926c-7923c78608c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min() and max() days patient is admitted.\n",
    "print(\"max days a patient admitted in train data\",train_inptn_data[\"Admit_For_Days\"].max())\n",
    "print(\"min days a patient addmitted in train data\",train_inptn_data[\"Admit_For_Days\"].min())\n",
    "print(\"max days a patient admitted in train data\",test_inptn_data[\"Admit_For_Days\"].max())\n",
    "print(\"min days a patient addmitted in train data\",test_inptn_data[\"Admit_For_Days\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39e8b3-0f6d-4115-859d-1330fceece28",
   "metadata": {},
   "source": [
    "### outpatient data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f8460-adc6-4d32-ac03-b2e504bfca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outptn_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ebfdb-8b8d-4bdf-9163-69972033a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outptn_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1987df-6592-482b-90af-0620679a0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding how many null values in the train and test of outpatient.\n",
    "print(\"train outpatient data null values:\\n\",train_outptn_data.isnull().sum())\n",
    "print(\"---\"*100)\n",
    "print(\"test outpatient data null values:\\n\",test_outptn_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a7b62-a8fd-4f3c-af92-1cb4b017f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train dataset shape:\",train.shape)\n",
    "print(\"train benificiary data shape:\",train_ben_data.shape)\n",
    "print(\"train inpatient data shape:\",train_inptn_data.shape)\n",
    "print(\"train outpatient data shape:\",train_outptn_data.shape)\n",
    "\n",
    "print(\"test dataset shape:\",test.shape)\n",
    "print(\"test benificiary data shape:\",test_ben_data.shape)\n",
    "print(\"test inpatient data shape:\",test_inptn_data.shape)\n",
    "print(\"test outpatient data shape:\",test_outptn_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bcab20-c53e-47d8-ac69-219ad4efa367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both inpatient and outpatient data to make it as a  single data\n",
    "train_all_ptn_data = pd.merge(train_inptn_data,train_outptn_data,\n",
    "                              on = ['BeneID', 'ClaimID', 'ClaimStartDt', 'ClaimEndDt', 'Provider',\n",
    "       'InscClaimAmtReimbursed', 'AttendingPhysician', 'OperatingPhysician',\n",
    "       'OtherPhysician', 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "       'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "       'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "       'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'ClmProcedureCode_1',\n",
    "       'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "       'ClmProcedureCode_5', 'ClmProcedureCode_6', 'DeductibleAmtPaid',\n",
    "       'ClmAdmitDiagnosisCode'],how = \"outer\")\n",
    "test_all_ptn_data = pd.merge(test_inptn_data,test_outptn_data,\n",
    "                              on = ['BeneID', 'ClaimID', 'ClaimStartDt', 'ClaimEndDt', 'Provider',\n",
    "       'InscClaimAmtReimbursed', 'AttendingPhysician', 'OperatingPhysician',\n",
    "       'OtherPhysician', 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "       'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "       'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "       'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'ClmProcedureCode_1',\n",
    "       'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "       'ClmProcedureCode_5', 'ClmProcedureCode_6', 'DeductibleAmtPaid',\n",
    "       'ClmAdmitDiagnosisCode'],how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47c56c-80d6-42fe-80cb-159b26a2371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_all_ptn_data.shape)\n",
    "print(test_all_ptn_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a39df4-098f-424b-ad7c-43659cf2dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_ptn_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598868d2-5256-4102-b37e-9ec8d181eacf",
   "metadata": {},
   "source": [
    "### **Merge beneficiary data to all patients data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a2d7d-4218-4cfe-bb7c-2487b78f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging beneciary details data to all patient data\n",
    "train_all_ptn_ben_data = pd.merge(train_all_ptn_data,train_ben_data,on = [\"BeneID\"],how = \"inner\")\n",
    "test_all_ptn_ben_data = pd.merge(test_all_ptn_data,test_ben_data,on = [\"BeneID\"],how = \"inner\")\n",
    "print(train_all_ptn_ben_data.shape)\n",
    "print(test_all_ptn_ben_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4a408-c656-443b-82cb-9a78d36cd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_ptn_ben_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52157f-c74e-4d35-b5da-94dc832c9936",
   "metadata": {},
   "source": [
    "### **merge potential fraud details of providers with patient data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4cba8d-5110-488c-bdc9-581ce0a3f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge train dataset(tells which provider fraud) with all patient details with beneficiary details.\n",
    "train_prov_with_patn_ben = pd.merge(train_all_ptn_ben_data,train,on = [\"Provider\"],how = \"inner\")\n",
    "test_prov_with_patn_ben = pd.merge(test_all_ptn_ben_data,test,on = [\"Provider\"],how = \"inner\")\n",
    "print(\"shape of the train provider details with all patients data:\",train_prov_with_patn_ben.shape)\n",
    "print(\"shape of the test provider details with all patients data:\",test_prov_with_patn_ben.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb361c75-6de8-43ca-a831-ed0be0be9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prov_with_patn_ben.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3616d-510b-4ff0-9ad9-cc5514267800",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prov_with_patn_ben.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa6c6e-c154-4e5b-b656-3f5086b85641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check null value percentage in each column for an entire dataset\n",
    "print(\"Null value percentage for train provider with patient details:\")\n",
    "print(train_prov_with_patn_ben.isnull().sum()*100/len(train_prov_with_patn_ben))\n",
    "print(\"---\"*100)\n",
    "print(\"Null value percentage for test provider with patient details:\")\n",
    "print(test_prov_with_patn_ben.isnull().sum()*100/len(test_prov_with_patn_ben))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bf78e-3ad9-4866-98ef-dd1e4025569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking a datatype of an both test and train provider with patient details dataset\n",
    "print(train_prov_with_patn_ben.dtypes)\n",
    "print(\"---\"*100)\n",
    "print(test_prov_with_patn_ben.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228f61a-e442-4117-bcb0-1c2ae48b8254",
   "metadata": {},
   "source": [
    "###  plot potential fraud class proportion in train with merged data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50726f94-6031-475b-9b98-8e60dfd2a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white',rc={'figure.figsize':(12,8)})\n",
    "#plotting potential fraud classes proportion in train with merged data\n",
    "classes_count = train_prov_with_patn_ben[\"PotentialFraud\"].value_counts()\n",
    "print(\"potential fraud distribution in percentage:\",classes_count*100/len(train_prov_with_patn_ben[\"PotentialFraud\"]))\n",
    "\n",
    "classes_count.plot(kind = \"bar\",figsize = (10,6),rot = 0,color = [\"blue\",\"red\"])\n",
    "plt.title(\"potential fraud distribution in train with merged data\")\n",
    "plt.xlabel(\"PotentialFraud\")\n",
    "plt.ylabel(\"No.of.potential fraud per class\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d14a2-c8b3-436d-b589-67697a8f623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting potentialfraud classes distribution in train data(only have providerid and he is fraud or not)\n",
    "count_classes_provider = train[\"PotentialFraud\"].value_counts()\n",
    "print(\"percentage of classes distribution in train data alone:\",count_classes_provider*100/len(train))\n",
    "\n",
    "count_classes_provider.plot(kind = \"bar\",rot = 0,figsize = (10,6),color = [\"blue\",\"red\"])#rot = 90 means rotate xticks in 90 degree that means it looks like vertical.\n",
    "plt.ylabel(\"No.of.potential fraud per class\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d12b26-6158-4b5b-b625-8ad897a1e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency distribution among beneficiary statewise.\n",
    "bene_count_statewise_per = (train_prov_with_patn_ben[\"State\"].value_counts())*100/len(train_prov_with_patn_ben[\"State\"])\n",
    "print(\"statewise beneficiary id distribution:\\n\",bene_count_statewise_per)\n",
    "#bar chart\n",
    "bene_count_statewise_per.plot(kind = \"bar\",figsize = (16,12),rot = 0)\n",
    "plt.yticks(range(0,10,2),(\"0%\",\"2%\",\"4%\",\"6%\",\"8%\"))\n",
    "plt.title(\"distribution of beneficiary id among states\",fontsize = 12)\n",
    "plt.xlabel(\"State ID\")\n",
    "plt.ylabel(\"Percentage of beneficiary %\")\n",
    "plt.show()\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7274341-29d2-4c8c-88f3-0731073ef1ee",
   "metadata": {},
   "source": [
    "### Racewise beneficiary distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61be80f-6c37-43c8-a857-686d94e5083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reacewise beneficaiary distribution\n",
    "count_race = (train_ben_data[\"Race\"].value_counts())*100/len(train_ben_data)\n",
    "print(\"Race wise beneficiary distribution:\\n\",count_race)\n",
    "\n",
    "count_race.plot(kind = \"bar\",rot = 0,figsize = (10,6))\n",
    "plt.title(\"Race-Wise beneficiary distribution\",fontsize = 12)\n",
    "plt.ylabel(\"Percentage of beneficiary distribution among each race\")\n",
    "plt.yticks(range(0,100,10),(\"0%\",\"10%\",\"20%\",\"30%\",\"40%\",\"50%\",\"60%\",\"70%\",\"80%\",\"90%\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628af869-6fba-43f4-a8c3-dcf1ac68a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prov_with_patn_ben[\"ClmProcedureCode_1\"].value_counts().iloc[:10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060571cd-69e2-487f-86b4-c42ffb6bc3dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#top 10 procedures in clmprocedurecode_1 that involve in heathcare fraud\n",
    "sns.countplot(x = \"ClmProcedureCode_1\",hue = \"PotentialFraud\",data = train_prov_with_patn_ben\n",
    "             ,order=train_prov_with_patn_ben.ClmProcedureCode_1.value_counts().iloc[:10].index)#without order it considers null values and creates big plot so because of that we can't able to plot a bar.\n",
    "plt.title(\"Top 10 clmprocedurecode_1 invoved in healthcare fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eeb8d9-7ddb-4abc-a60d-5829031f3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 10 count of ClmDiagnosisCode_1 involed in healthcare fraud.\n",
    "sns.countplot(x = \"ClmDiagnosisCode_1\",hue = \"PotentialFraud\",data = train_prov_with_patn_ben\n",
    "              ,order = train_prov_with_patn_ben.ClmDiagnosisCode_1.value_counts().iloc[0:10].index)\n",
    "plt.title(\"Top 10 ClmDiagnosisCode_1 count involved in healthcare fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5c0c3-61de-41a0-a3f5-18efe47ab76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top-10 AttendingPhysician  involved in halthcarefraud\n",
    "plt.figure(figsize = (16,12))\n",
    "sns.countplot(x = \"AttendingPhysician\",hue = \"PotentialFraud\",data = train_prov_with_patn_ben\n",
    "              ,order = train_prov_with_patn_ben[\"AttendingPhysician\"].value_counts().iloc[:20].index)\n",
    "plt.title(\"Top 10 attending physicians involved in fraud\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642739f9-eaea-43b3-8a51-88fe2b5d09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_prov_with_patn_ben.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef29307-b5fe-43a8-894a-cc9e53375424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPAnnualReimbursementAmt vs IPAnnualDeductibleAmt\n",
    "sns.set(rc={'figure.figsize':(12,8)},style='dark')\n",
    "sns.lmplot(x = \"IPAnnualDeductibleAmt\",y = \"IPAnnualReimbursementAmt\",data = train_prov_with_patn_ben\n",
    "           ,hue = \"PotentialFraud\",col = \"PotentialFraud\",fit_reg = False)\n",
    "plt.show()\n",
    "\"\"\"There is no visible difference in the graph\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eab70e-83ca-4972-a62c-fc1a11d71d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeductibleAmtPaid vs InscClaimAmtReimbursed in both fraud and non fraud\n",
    "sns.lmplot(y = \"InscClaimAmtReimbursed\",x = \"DeductibleAmtPaid\",data = train_prov_with_patn_ben\n",
    "           ,hue = \"PotentialFraud\",col = \"PotentialFraud\",fit_reg = False)\n",
    "plt.show()\n",
    "\"\"\"DeductibleAmtPaid vs InscClaimAmtReimbursed for both fraud and non fraud looks like very same\n",
    "we can't able to differentiate\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02ad04-41bf-4142-8829-d405fde445ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets check age vs InscClaimAmtReimbursed in both fraud and non fraud\n",
    "plt.figure(figsize = (16,12))\n",
    "plt.subplot(2,1,1)\n",
    "x = train_prov_with_patn_ben[train_prov_with_patn_ben.PotentialFraud == \"Yes\"].Age\n",
    "y = train_prov_with_patn_ben[train_prov_with_patn_ben.PotentialFraud == \"Yes\"].InscClaimAmtReimbursed\n",
    "plt.scatter(x,y)\n",
    "plt.title(\"Fraud\")\n",
    "plt.ylabel(\"Insurance Claim Amout Reimbursed\")\n",
    "plt.xlabel('Age (in Years)')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "x = train_prov_with_patn_ben[train_prov_with_patn_ben.PotentialFraud == \"Yes\"].Age\n",
    "y = train_prov_with_patn_ben[train_prov_with_patn_ben.PotentialFraud == \"Yes\"].InscClaimAmtReimbursed\n",
    "plt.title(\"Non - Fraud\")\n",
    "plt.ylabel(\"Insurance Claim Amout Reimbursed\")\n",
    "plt.xlabel('Age (in Years)')\n",
    "plt.scatter(x,y)\n",
    "\n",
    "plt.suptitle(\"Age vs Insurance Claim Amount Reimbursed (Fraud vs Non-Fraud)\",fontsize = 16,y=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd51121-e7d4-4f99-ba94-10f3044e9d58",
   "metadata": {},
   "source": [
    "### **Appending Train data to Test data will help you get good average scores of new features in Test data,as we see not all levels of variables are present in test data compared to train data.So our approach here will be-to append train data to test data ,derive new average features and take only test data to evaluate results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0342b-cf74-4c0c-8649-9d4e9032bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before appending the train data to the test data,we are creating a copy of test_data for future use.\n",
    "test_prov_with_patn_ben_copy = test_prov_with_patn_ben\n",
    "print(\"test_prov_with_patn_ben shape:\",test_prov_with_patn_ben.shape)\n",
    "print(\"test_prov_with_patn_ben_copy:\",test_prov_with_patn_ben_copy.shape)\n",
    "#As the test data doesn't have a target columns because of that we are only taking the columns that exists in the train data.\n",
    "cols_test = test_prov_with_patn_ben.columns\n",
    "#now appending the train data to test data\n",
    "test_prov_with_patn_ben = pd.concat([test_prov_with_patn_ben,train_prov_with_patn_ben[cols_test]])\n",
    "print(\"After combining both train and test expected no of collumns in test data:\",(test_prov_with_patn_ben_copy.shape[0]) + (train_prov_with_patn_ben.shape[0]))\n",
    "print(\"Total no of columns in test data :\",test_prov_with_patn_ben.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb5405-3910-4f53-aa2c-6ecbc05c2318",
   "metadata": {},
   "source": [
    "### Using groupby we calculate a average based on each category in the column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf80a6-b7f9-400e-bce5-068aa95ae041",
   "metadata": {},
   "source": [
    "***Avg features grouped by Provider***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff9a74-2759-49ed-832f-80168c2a9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we group the entire dataset based on Provider.\n",
    "# columns that we are going to calculate a average.\n",
    "avg_cols = ['InscClaimAmtReimbursed', 'DeductibleAmtPaid', 'IPAnnualReimbursementAmt', \n",
    "           'IPAnnualDeductibleAmt', 'OPAnnualReimbursementAmt', 'OPAnnualDeductibleAmt',\n",
    "           'Age', 'NoOfMonths_PartACov', 'NoOfMonths_PartBCov', 'Admit_For_Days']\n",
    "\n",
    "# Using for loop we are calculating a average for each category in the column\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerProviderAvg_{col}\"] = train_prov_with_patn_ben.groupby('Provider')[col].transform('mean')\n",
    " \n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerProviderAvg_{col}\"] = test_prov_with_patn_ben.groupby('Provider')[col].transform('mean')\n",
    "\n",
    "#As we know we are created a new column 10 columns that are the averages of the aldready existing columns.\n",
    "#so the new columns are getting added at the end of the dataset.\n",
    "print(\"Test:\",test_prov_with_patn_ben.shape)\n",
    "print(test_prov_with_patn_ben.iloc[:,-10:].head(2))\n",
    "print(\"Train:\",train_prov_with_patn_ben.shape)\n",
    "print(train_prov_with_patn_ben.iloc[:,-10:].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351480a-c179-4500-a10e-98178db8f093",
   "metadata": {},
   "source": [
    "***Avg features grouped by BeneID***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c821f-fa11-4cb0-b421-c74931c151c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we group the entire dataset based on BeneID.\n",
    "# columns that we are going to calculate a average.\n",
    "avg_cols = ['InscClaimAmtReimbursed', 'DeductibleAmtPaid', 'IPAnnualReimbursementAmt', \n",
    "                'IPAnnualDeductibleAmt', 'OPAnnualReimbursementAmt', 'OPAnnualDeductibleAmt',\n",
    "                'Admit_For_Days']\n",
    "\n",
    "# Create all per-beneficiary features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerBeneIDAvg_{col}\"] = train_prov_with_patn_ben.groupby('BeneID')[col].transform('mean')\n",
    "\n",
    "# Create all per-beneficiary features for test  \n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerBeneIDAvg_{col}\"] = test_prov_with_patn_ben.groupby('BeneID')[col].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfbd61e-6847-4426-9291-4707f46dca6a",
   "metadata": {},
   "source": [
    "***Avg features grouped by other physician***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a54912-1891-4791-aa0f-fa6ecf4b3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we group the entire dataset based on other physician\n",
    "\n",
    "# Create all per-other-physician features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerOtherPhysicianAvg_{col}\"] = train_prov_with_patn_ben.groupby('OtherPhysician')[col].transform('mean')\n",
    "\n",
    "# Create all per-other-physician features for test  \n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerOtherPhysicianAvg_{col}\"] = test_prov_with_patn_ben.groupby('OtherPhysician')[col].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85262fca-ff0b-4b97-8076-aefe00877359",
   "metadata": {},
   "source": [
    "***Avg features grouped by operating physician***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0195e0-3454-45b5-aa2a-495e7276ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset groped based on operating physician\n",
    "\n",
    "# Create all per-operating-physician features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerOperatingPhysicianAvg_{col}\"] = train_prov_with_patn_ben.groupby('OperatingPhysician')[col].transform('mean')\n",
    "\n",
    "# Create all per-operating-physician features for test  \n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerOperatingPhysicianAvg_{col}\"] = test_prov_with_patn_ben.groupby('OperatingPhysician')[col].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792557fe-ccaf-4b36-9d01-53402f47183a",
   "metadata": {},
   "source": [
    "***Avg features grouped by attending physician***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ca7f5-a83d-408f-b7e9-5214ccc12165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping based on attending physician\n",
    "\n",
    "# Create all per-attending-physician features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerAttendingPhysicianAvg_{col}\"] = train_prov_with_patn_ben.groupby('AttendingPhysician')[col].transform('mean')\n",
    "\n",
    "# Create all per-attending-physician features for test  \n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerAttendingPhysicianAvg_{col}\"] = test_prov_with_patn_ben.groupby('AttendingPhysician')[col].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3126f-4127-468d-b18e-bfffeda356e8",
   "metadata": {},
   "source": [
    "***Avg features grouped by diagnosis group code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f89dec-450d-427a-a6a3-98059fb6b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping based on diagnosis group code\n",
    "\n",
    "# Create all per-diagnosis-group-code features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerDiagnosisGroupCodeAvg_{col}\"] = train_prov_with_patn_ben.groupby('DiagnosisGroupCode')[col].transform('mean')\n",
    "\n",
    "# Create all per-diagnosis-group-code features for test  \n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerDiagnosisGroupCodeAvg_{col}\"] = test_prov_with_patn_ben.groupby('DiagnosisGroupCode')[col].transform('mean')\n",
    "\n",
    "# Verify diagnosis group code features were created\n",
    "diagnosis_features = [col for col in train_prov_with_patn_ben.columns if col.startswith('PerDiagnosisGroupCodeAvg_')]\n",
    "print(f\"Created {len(diagnosis_features)} per-diagnosis-group-code average features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6eea3-c2c1-4f17-b0a4-cc5ec29d0af1",
   "metadata": {},
   "source": [
    "***Avg features grouped by admit diagnosis code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af5e40-b9ad-46d7-be20-3cee9579ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping based on admit diagnosis code\n",
    "\n",
    "# Create all per-claim-admit-diagnosis-code features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerClmAdmitDiagnosisCodeAvg_{col}\"] = train_prov_with_patn_ben.groupby('ClmAdmitDiagnosisCode')[col].transform('mean')\n",
    "\n",
    "# Create all per-claim-admit-diagnosis-code features for test  \n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerClmAdmitDiagnosisCodeAvg_{col}\"] = test_prov_with_patn_ben.groupby('ClmAdmitDiagnosisCode')[col].transform('mean')\n",
    "\n",
    "# Verify claim admit diagnosis code features were created\n",
    "clm_admit_features = [col for col in train_prov_with_patn_ben.columns if col.startswith('PerClmAdmitDiagnosisCodeAvg_')]\n",
    "print(f\"Created {len(clm_admit_features)} per-claim-admit-diagnosis-code average features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f268ddc-a233-48e5-b201-eacaa7464dcd",
   "metadata": {},
   "source": [
    "***Avg features grouped by claim procedure code 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d7c47-c920-4b50-b46d-0c472ea99325",
   "metadata": {},
   "outputs": [],
   "source": [
    "### grouped based on per claim procedure code 1\n",
    "\n",
    "# Create all per-claim-procedure-code-1 features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerClmProcedureCode_1Avg_{col}\"] = train_prov_with_patn_ben.groupby('ClmProcedureCode_1')[col].transform('mean')\n",
    "\n",
    "# Create all per-claim-procedure-code-1 features for test  \n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerClmProcedureCode_1Avg_{col}\"] = test_prov_with_patn_ben.groupby('ClmProcedureCode_1')[col].transform('mean')\n",
    "\n",
    "# Verify claim procedure code 1 features were created\n",
    "procedure_1_features = [col for col in train_prov_with_patn_ben.columns if col.startswith('PerClmProcedureCode_1Avg_')]\n",
    "print(f\"Created {len(procedure_1_features)} per-claim-procedure-code-1 average features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135612a-2d95-40be-9262-825064b10ecf",
   "metadata": {},
   "source": [
    "***Avg features grouped by diagnosis group code 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddcddfa-7bbd-4049-a880-b0e570cf1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### grouped based on claim procedure code 2\n",
    "\n",
    "# Create all per-claim-procedure-code-2 features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerClmProcedureCode_2Avg_{col}\"] = train_prov_with_patn_ben.groupby('ClmProcedureCode_2')[col].transform('mean')\n",
    "\n",
    "# Create all per-claim-procedure-code-2 features for test\n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerClmProcedureCode_2Avg_{col}\"] = test_prov_with_patn_ben.groupby('ClmProcedureCode_2')[col].transform('mean')\n",
    "\n",
    "# Verify claim procedure code 2 features were created\n",
    "procedure_2_features = [col for col in train_prov_with_patn_ben.columns if col.startswith('PerClmProcedureCode_2Avg_')]\n",
    "print(f\"Created {len(procedure_2_features)} per-claim-procedure-code-2 average features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69220614-a59d-40ba-b576-73ddd16ed5ca",
   "metadata": {},
   "source": [
    "***Avg features grouped by diagnosis group code 3***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d24d86-1fe9-4391-95c0-4c97dca9bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### grouped based on claim procedure code 3\n",
    "\n",
    "# Create all per-claim-procedure-code-3 features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerClmProcedureCode_3Avg_{col}\"] = (\n",
    "        train_prov_with_patn_ben.groupby('ClmProcedureCode_3')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "# Create all per-claim-procedure-code-3 features for test\n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerClmProcedureCode_3Avg_{col}\"] = (\n",
    "        test_prov_with_patn_ben.groupby('ClmProcedureCode_3')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "# Verify claim procedure code 3 features were created\n",
    "procedure_3_features = [col for col in train_prov_with_patn_ben.columns \n",
    "                        if col.startswith('PerClmProcedureCode_3Avg_')]\n",
    "print(f\"Created {len(procedure_3_features)} per-claim-procedure-code-3 average features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6c15e-91e0-4a07-927c-5dccec5688d5",
   "metadata": {},
   "source": [
    "***Avg features grouped by claim diagnosis code 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691d607-a932-4974-9967-725176973b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### grouped based on claim diagnosis code 1\n",
    "\n",
    "# Create all per-claim-diagnosis-code-1 features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerClmDiagnosisCode_1Avg_{col}\"] = (\n",
    "        train_prov_with_patn_ben.groupby('ClmDiagnosisCode_1')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "# Create all per-claim-diagnosis-code-1 features for test\n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerClmDiagnosisCode_1Avg_{col}\"] = (\n",
    "        test_prov_with_patn_ben.groupby('ClmDiagnosisCode_1')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "# Verify claim diagnosis code 1 features were created\n",
    "diagnosis_1_features = [col for col in train_prov_with_patn_ben.columns \n",
    "                        if col.startswith('PerClmDiagnosisCode_1Avg_')]\n",
    "print(f\"Created {len(diagnosis_1_features)} per-claim-diagnosis-code-1 average features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9936af-81e5-4d0a-bf7b-0edb7461d89b",
   "metadata": {},
   "source": [
    "***Average features grouped by ClmDiagnosisCode_2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a155846-9f2f-4c7d-a49c-aec0ed03e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average features grouped by ClmDiagnosisCode_2\n",
    "\n",
    "# Create all per-claim-diagnosis-code-2 features for train\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f\"PerClmDiagnosisCode_2Avg_{col}\"] = (\n",
    "        train_prov_with_patn_ben.groupby('ClmDiagnosisCode_2')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "# Create all per-claim-diagnosis-code-2 features for test  \n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f\"PerClmDiagnosisCode_2Avg_{col}\"] = (\n",
    "        test_prov_with_patn_ben.groupby('ClmDiagnosisCode_2')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "# Verify claim diagnosis code 2 features were created\n",
    "diagnosis_2_features = [col for col in train_prov_with_patn_ben.columns \n",
    "                       if col.startswith('PerClmDiagnosisCode_2Avg_')]\n",
    "print(f\"Created {len(diagnosis_2_features)} per-claim-diagnosis-code-2 average features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7176f-f8ad-4765-a84e-01a3d59d9d61",
   "metadata": {},
   "source": [
    "***Average features grouped by ClmDiagnosisCode_3 and ClmDiagnosisCode_4***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e103a6-38bd-4c1a-b48b-a8af91c1f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average features grouped by ClmDiagnosisCode_3\n",
    "\n",
    "# Using predefined avg_cols list\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f'ClmDiagnosisCode_3_{col}_avg'] = (\n",
    "        train_prov_with_patn_ben.groupby('ClmDiagnosisCode_3')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f'ClmDiagnosisCode_3_{col}_avg'] = (\n",
    "        test_prov_with_patn_ben.groupby('ClmDiagnosisCode_3')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "# Verify claim diagnosis code 3 features were created\n",
    "diagnosis_3_features = [\n",
    "    col for col in train_prov_with_patn_ben.columns\n",
    "    if col.startswith('ClmDiagnosisCode_3') and col.endswith('_avg')\n",
    "]\n",
    "print('Created', len(diagnosis_3_features), 'per-claim-diagnosis-code-3 average features')\n",
    "\n",
    "\n",
    "# Average features grouped by ClmDiagnosisCode_4\n",
    "# Using predefined avg_cols list\n",
    "for col in avg_cols:\n",
    "    train_prov_with_patn_ben[f'ClmDiagnosisCode_4_{col}_avg'] = (\n",
    "        train_prov_with_patn_ben.groupby('ClmDiagnosisCode_4')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "for col in avg_cols:\n",
    "    test_prov_with_patn_ben[f'ClmDiagnosisCode_4_{col}_avg'] = (\n",
    "        test_prov_with_patn_ben.groupby('ClmDiagnosisCode_4')[col].transform('mean')\n",
    "    )\n",
    "\n",
    "# Verify claim diagnosis code 4 features were created\n",
    "diagnosis_4_features = [\n",
    "    col for col in train_prov_with_patn_ben.columns\n",
    "    if col.startswith('ClmDiagnosisCode_4') and col.endswith('_avg')\n",
    "]\n",
    "print('Created', len(diagnosis_4_features), 'per-claim-diagnosis-code-4 average features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48541016-d4aa-4d94-9eef-8a1a358cf74a",
   "metadata": {},
   "source": [
    "***Claims are filed by Provider,so fraud can be organized crime.So we will check ClmCounts filed by Providers and when pairs like Provider +BeneID, Provider+Attending Physician, Provider+ClmAdmitDiagnosisCode, Provider+ClmProcedureCode_1,Provider+ClmDiagnosisCode_1 are together.\n",
    "Average Feature based on grouping based on combinations of different variables.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedede3-f2c6-4dd7-9d62-04e7713282ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grouping based on different combinations and make count of them based on claim ID for both train and test data.\n",
    "\n",
    "# 1) Define your partner cols & explicit combos (same as before)\n",
    "two_way_feats = [\n",
    "    'BeneID', 'AttendingPhysician', 'OtherPhysician', 'OperatingPhysician',\n",
    "    'ClmAdmitDiagnosisCode',\n",
    "    'ClmProcedureCode_1','ClmProcedureCode_2','ClmProcedureCode_3',\n",
    "    'ClmProcedureCode_4','ClmProcedureCode_5',\n",
    "    'ClmDiagnosisCode_1','ClmDiagnosisCode_2','ClmDiagnosisCode_3',\n",
    "    'ClmDiagnosisCode_4','ClmDiagnosisCode_5','ClmDiagnosisCode_6',\n",
    "    'ClmDiagnosisCode_7','ClmDiagnosisCode_8','ClmDiagnosisCode_9',\n",
    "    'DiagnosisGroupCode'\n",
    "]\n",
    "\n",
    "three_way_feats = [\n",
    "    ['BeneID','AttendingPhysician'],\n",
    "    ['BeneID','OtherPhysician'],\n",
    "    ['BeneID','OperatingPhysician'],\n",
    "    ['BeneID','ClmProcedureCode_1'],\n",
    "    ['BeneID','ClmDiagnosisCode_1']\n",
    "]\n",
    "\n",
    "four_way_feats = [\n",
    "    ['BeneID','AttendingPhysician','ClmProcedureCode_1'],\n",
    "    ['BeneID','AttendingPhysician','ClmDiagnosisCode_1'],\n",
    "    ['BeneID','ClmDiagnosisCode_1','ClmProcedureCode_1']\n",
    "]\n",
    "\n",
    "# 2) Build your full list of groupâ€by keyâ€lists\n",
    "groupings = []\n",
    "groupings.append(['Provider'])                                   # 1â€way\n",
    "groupings += [['Provider', c] for c in two_way_feats]             # 2â€way\n",
    "groupings += [['Provider'] + combo for combo in three_way_feats]  # 3â€way\n",
    "groupings += [['Provider'] + combo for combo in four_way_feats]   # 4â€way\n",
    "\n",
    "# 3) Loop once to add all ClmCount_â€¦ features into both train & test\n",
    "for grp in groupings:\n",
    "    col_name = 'ClmCount_' + '_'.join(grp)\n",
    "    train_prov_with_patn_ben[col_name] = (\n",
    "        train_prov_with_patn_ben.groupby(grp)['ClaimID']\n",
    "                               .transform('count')\n",
    "    )\n",
    "    test_prov_with_patn_ben[col_name] = (\n",
    "        test_prov_with_patn_ben.groupby(grp)['ClaimID']\n",
    "                               .transform('count')\n",
    "    )\n",
    "\n",
    "print(f\" Created {len(groupings)} ClmCount_â€¦ features on train & test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035f6da-70a6-4c25-8d93-7501877e3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of both train and test data\n",
    "print(\"train_prov_with_patn_ben shape :\",train_prov_with_patn_ben.shape)\n",
    "print(\"test_prov_with_patn_ben shape :\",test_prov_with_patn_ben.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02776f88-81ba-44c7-8002-78abfbc02102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the unique values in the diagnosis code 1 column\n",
    "# we are forcing converting the each values in the column into str type because if it is a Nan or othe values exits it throughs an error.\n",
    "diagnosis_code1 = train_prov_with_patn_ben[\"ClmDiagnosisCode_1\"].astype(str).str[0:2]\n",
    "print(diagnosis_code1.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd03895-f35a-4f1a-90bd-bb6849acae41",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552fe49a-03c0-4d1d-ba03-66f76184dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling a Numeric column null values with 0\n",
    "col_num = train_prov_with_patn_ben.select_dtypes([np.number]).columns #np.number is a alias for all numerical datatypes.\n",
    "train_prov_with_patn_ben[col_num] = train_prov_with_patn_ben[col_num].fillna(0)\n",
    "test_prov_with_patn_ben[col_num] = test_prov_with_patn_ben[col_num].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c01373-ee06-4bd0-940f-29cb762cdb5b",
   "metadata": {},
   "source": [
    "***Feature Selection***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fcf44e-5764-4002-b45a-6ecbf088ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we extracted all informations as numerical values now we are going to drop the original columns from the data.\n",
    "#we do this because we extracted all the usefull informations from this original column that column is become useless now.\n",
    "cols = train_prov_with_patn_ben.columns\n",
    "cols[0:58]\n",
    "\n",
    "remove_these_columns=['BeneID', 'ClaimID', 'ClaimStartDt','ClaimEndDt','AttendingPhysician',\n",
    "       'OperatingPhysician', 'OtherPhysician', 'ClmDiagnosisCode_1',\n",
    "       'ClmDiagnosisCode_2', 'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4',\n",
    "       'ClmDiagnosisCode_5', 'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7',\n",
    "       'ClmDiagnosisCode_8', 'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10',\n",
    "       'ClmProcedureCode_1', 'ClmProcedureCode_2', 'ClmProcedureCode_3',\n",
    "       'ClmProcedureCode_4', 'ClmProcedureCode_5', 'ClmProcedureCode_6',\n",
    "       'ClmAdmitDiagnosisCode', 'AdmissionDt',\n",
    "       'DischargeDt', 'DiagnosisGroupCode','DOB', 'DOD',\n",
    "        'State', 'County']\n",
    "\n",
    "train_category_removed=train_prov_with_patn_ben.drop(axis=1,columns=remove_these_columns)\n",
    "test_category_removed=test_prov_with_patn_ben.drop(axis=1,columns=remove_these_columns)\n",
    "print(\"train shape:\",train_category_removed.shape)\n",
    "print(\"test shape:\",test_category_removed.shape)\n",
    "print(\"train null values:\",train_category_removed.isnull().sum().sum())\n",
    "print(\"test null values:\",test_category_removed.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ea70a-50b0-4362-bdde-aa6bb9853738",
   "metadata": {},
   "source": [
    "***One Hot Encoding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84484a6-95bc-4b97-a599-a33388b051e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for features race and gender we are going to done a one hot encoding.\n",
    "#Type conversion\n",
    "train_category_removed[\"Gender\"] = train_category_removed[\"Gender\"].astype(\"category\")\n",
    "test_category_removed[\"Gender\"] = test_category_removed[\"Gender\"].astype(\"category\")\n",
    "train_category_removed[\"Race\"] = train_category_removed[\"Race\"].astype(\"category\")\n",
    "test_category_removed[\"Race\"] = test_category_removed[\"Race\"].astype(\"category\")\n",
    "\n",
    "#Dummification\n",
    "train_category_removed = pd.get_dummies(train_category_removed,columns = [\"Gender\",\"Race\"],drop_first = True)\n",
    "test_category_removed = pd.get_dummies(test_category_removed,columns = [\"Gender\",\"Race\"],drop_first = True)\n",
    "\n",
    "#checking the columns\n",
    "train_category_removed.head().T #T denotes transpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad37c63-4852-4aed-be5e-b68943f1f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_category_removed.iloc[135391:135393]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1ea73-04e0-4eb2-a1cc-42426a3f75ab",
   "metadata": {},
   "source": [
    "***Converting Target values Yes->0 & No->1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd42f9d-3752-43e3-af8d-be704e6415b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using map function\n",
    "train_category_removed[\"PotentialFraud\"] = train_category_removed[\"PotentialFraud\"].map({\"Yes\":1,\"No\":0}).astype('int64')\n",
    "print(train_category_removed.dtypes)\n",
    "print(\"train_category_removed potential fraud min value:\",train_category_removed[\"PotentialFraud\"].min())\n",
    "print(\"train_category_removed potential fraud min value:\",train_category_removed[\"PotentialFraud\"].max())\n",
    "train_category_removed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683d6dc-614b-4d0c-8f0d-b2bc6a7fdbb4",
   "metadata": {},
   "source": [
    "***Now we are removing the appended train data from the test data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0d93b-abe9-4437-bb84-d447e558c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of test data before removing appended train data :\",test_category_removed.shape)\n",
    "#updating test data by removing appended train data.\n",
    "test_category_removed = test_category_removed.iloc[:135392]\n",
    "print(\"shape of test data after removed appended train data :\",test_category_removed.shape)\n",
    "#checking the last values to confirm that the appended train data has been removed.\n",
    "test_category_removed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc96d84-4d16-4b15-8af9-a934a8f4fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_category_removed.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae261f6-7912-46b1-b0cd-2afc06460a1a",
   "metadata": {},
   "source": [
    "***Aggregation to the prviders level***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a405b9-c04b-49d3-bfa8-f10ed2347afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.rows\",None)\n",
    "print(train_category_removed.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b75a5-0f8c-4820-94d3-38d7e5e443bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets aggregate based on the provider and potentialfraud for train data & provider for test.\n",
    "#it includes all features except reneldiseaseindicator because it contains string value. \n",
    "train_category_removed_groupedby_prov = train_category_removed.groupby([\"Provider\",\"PotentialFraud\"],as_index = False).sum(numeric_only = True)\n",
    "test_category_removed_groupedby_prov = test_category_removed.groupby([\"Provider\"],as_index = False).sum(numeric_only = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bccb8e8-c0ee-43c5-bd9c-7cade985c88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_category_removed_groupedby_prov.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b98b5b-1e9e-40b5-9896-7da172e1ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Providers in train:\",train_category_removed_groupedby_prov.shape)\n",
    "print(\"Providers in test:\",test_category_removed_groupedby_prov.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdc3e0-9c6a-401c-9992-51c4c4197d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate x & y data\n",
    "x = train_category_removed_groupedby_prov.drop([\"Provider\",\"PotentialFraud\"],axis = 1)\n",
    "y = train_category_removed_groupedby_prov[\"PotentialFraud\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8f80e-dab3-40dc-bcbf-bd84115fe1a3",
   "metadata": {},
   "source": [
    "***Feature Scaling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d72fa-9f34-497f-9b12-6a7ca3b0ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardizing a \"x\" data.\n",
    "sc = StandardScaler()\n",
    "sc.fit(x)\n",
    "x_std = sc.transform(x)\n",
    "\n",
    "x_test_std = sc.transform(test_category_removed_groupedby_prov.iloc[:,1:])\n",
    "print(x_test_std[:3,:])\n",
    "print(x_std[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9718214-bd26-4119-91d5-7dc6f74b970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape :\",x_std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4973fa-6973-4518-83ef-6cedaa3d33f9",
   "metadata": {},
   "source": [
    "***Lets split the train and test data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ffe83-390a-4850-8430-734b67301032",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(x_std,y,test_size = 0.3,random_state = 42,stratify = y,shuffle = True)\n",
    "print(\"x train shape:\",x_train.shape)\n",
    "print(\"x val shape:\",x_val.shape)\n",
    "print(\"x train shape:\",y_train.shape)\n",
    "print(\"y val shape:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119aac9-dea6-43ea-8094-87d5c22e39bc",
   "metadata": {},
   "source": [
    "### ***Model Building***\n",
    "***Logistic Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c73c8a-f85c-431b-91ad-ff04e8532e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logisticregressioncv for choosing a best regularization parameter c.\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "log_mod = LogisticRegressionCV(cv = 10,class_weight = \"balanced\",random_state = 42)\n",
    "\"\"\"class_weight = \"balanced\" is for giving balanced weightage for all the clases even if the one class is count wise\n",
    "very lower than the another class.\"\"\"\n",
    "log_mod.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd6214-847c-494d-841c-4dd0d8d59fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets predict the probability of 0 and 1 for both x_train and y_train.To check the model is overfitted or underfitted.\n",
    "x_train_pred_prob = log_mod.predict_proba(x_train)\n",
    "x_val_pred_prob = log_mod.predict_proba(x_val)\n",
    "print(x_train_pred_prob[:3])\n",
    "print(x_val_pred_prob[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07342d-2c76-431b-8f37-4795a09f300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the model is overfitted or underfitted using distplot\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.distplot(x_train_pred_prob[:,1])#we are only taking a 1's probability.\n",
    "sns.distplot(x_val_pred_prob[:,1])\n",
    "plt.title(\"Probability of 1 become prediction in both x_train and y_val\")\n",
    "plt.xlim([0,1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab4387-767d-43c2-8cb1-aa62750cc7a7",
   "metadata": {},
   "source": [
    "#### Logistic Regression : ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa98b1-e5f3-4d49-b805-f80815e04633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc,precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f20ff9-5731-49a9-bf74-849f853adcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc_curve tests a different threshold and returns threshold value and their respective fpr and tpr.\n",
    "fpr, tpr, thresholds = roc_curve(y_val,x_val_pred_prob[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr,label = f'ROC Curve (AUC = {roc_auc:.2f}%)',lw = 1)\n",
    "plt.plot([0,1],[0,1],ls = \"--\")\n",
    "\n",
    "for label in range(1,10,1):\n",
    "    x = (10 - label)/10\n",
    "    y = (10 - label)/10\n",
    "    plt.text(x,y,thresholds[label*15],fontdict={'size': 14})\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.01])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc16f7-3b12-4a8c-b9e1-2f7b053f9c56",
   "metadata": {},
   "source": [
    "***Logistic Regression : Precision VS Recall curve***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766bd25-17b7-4ab6-bbe2-6926681a0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are actually considering a fraud(1) positives only\n",
    "precision,recall,threshold = precision_recall_curve(y_val,x_val_pred_prob[:,1])\n",
    "plt.plot(precision,recall)\n",
    "plt.xlabel(\"precision\")\n",
    "plt.ylabel(\"recall\")\n",
    "plt.title(\"precision vs recall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ae61b-89d0-41a5-b8c9-b4be2a2474f1",
   "metadata": {},
   "source": [
    "***Checking the density of both Tpr and fpr***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4eb23-b05b-4f65-8447-5d39d8cd5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tpr)\n",
    "sns.distplot(fpr)\n",
    "plt.title(\"tpr vs fpr\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc8b05-e0ac-4e68-9c2b-81910b7489f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a7094-e766-4a28-8497-2d6cfcddba86",
   "metadata": {},
   "source": [
    "***Setting a probability threshold as 0.6***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff27a67-96ed-4fb3-8a1d-dfeb304d4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the threshold we find the fraud or not fraud\n",
    "log_train_pred = (x_train_pred_prob[:,1] > 0.60)\n",
    "log_val_pred = (x_val_pred_prob[:,1] > 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f65d2d-fe27-4ab6-882f-3bf13bea3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,precision_score,recall_score,roc_auc_score\n",
    "\n",
    "cm_train = confusion_matrix(y_train,log_train_pred,labels = [1,0])\n",
    "cm_val = confusion_matrix(y_val,log_val_pred,labels = [1,0])\n",
    "print(\"x train data confusion matrix:\\n\",cm_train)\n",
    "print(\"x validation data confusion matrix:\\n\",cm_val)\n",
    "\n",
    "accuracy_x_train = accuracy_score(y_train,log_train_pred)\n",
    "accuracy_x_val = accuracy_score(y_val,log_val_pred)\n",
    "print(\"x train data accuracy score:\",accuracy_x_train)\n",
    "print(\"x val data accuracy score:\",accuracy_x_val)\n",
    "\n",
    "prec_x_train = precision_score(y_train,log_train_pred)\n",
    "prec_x_val = precision_score(y_val,log_val_pred)\n",
    "print(\"x train data precision score:\",prec_x_train)\n",
    "print(\"x val data precision score:\",prec_x_val)\n",
    "\n",
    "recall_x_train = recall_score(y_train,log_train_pred)\n",
    "recall_x_val = recall_score(y_val,log_val_pred)\n",
    "print(\"x train data recall score:\",recall_x_train)\n",
    "print(\"x val data recall score:\",recall_x_val)\n",
    "\n",
    "f1_x_train = f1_score(y_train,log_train_pred)\n",
    "f1_x_val = f1_score(y_val,log_val_pred)\n",
    "print(\"x train data f1 score:\",f1_x_train)\n",
    "print(\"x val data f1 score:\",f1_x_val)\n",
    "\n",
    "print(\"area under curve for x_train data:\",roc_auc_score(y_train,log_train_pred))\n",
    "print(\"area under curve for x_val data:\",roc_auc_score(y_val,log_val_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f20430-9432-47c1-b909-a7a4180e80c0",
   "metadata": {},
   "source": [
    " ***Saving Logistic regression model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9278fc-e432-4f08-96ca-83f697fee1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Logistic Regression model\n",
    "joblib.dump(log_mod, 'logistic_regression_threshold_60.joblib')\n",
    "joblib.dump(sc, 'logistic_regression_scaler.joblib')\n",
    "\n",
    "# Save performance metrics\n",
    "log_metrics = {\n",
    "    'model_type': 'LogisticRegression',\n",
    "    'threshold': 0.60,\n",
    "    'train_accuracy': accuracy_x_train,\n",
    "    'val_accuracy': accuracy_x_val,\n",
    "    'train_f1': f1_x_train,\n",
    "    'val_f1': f1_x_val\n",
    "}\n",
    "\n",
    "with open('logistic_regression_metadata.json', 'w') as f:\n",
    "    json.dump(log_metrics, f, indent=2)\n",
    "\n",
    "# Test loading\n",
    "loaded_log_model = joblib.load('logistic_regression_threshold_60.joblib')\n",
    "loaded_log_scaler = joblib.load('logistic_regression_scaler.joblib')\n",
    "print(\"Logistic Regression saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ec6fa-f713-4a78-8b62-4d80c2f4e6d0",
   "metadata": {},
   "source": [
    "***Test data prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9235e-5b00-487b-a47e-cda926d31d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data prediction\n",
    "log_test_pred = (log_mod.predict_proba(x_test_std)[:,1] > 0.60)\n",
    "\"\"\"we convert it into a dataframe and replace the 1->yes and 0->no,then combine provider id with the predicted fraud or not\"\"\"\n",
    "log_test_pred = pd.DataFrame(log_test_pred)\n",
    "print(log_test_pred.head())\n",
    "#replacing the value 1 with \"Yes\" and 0 with \"No\"\n",
    "replacement = {1:\"Yes\",0:\"No\"}\n",
    "labels = log_test_pred[0].apply(lambda x:replacement[x])\n",
    "print(labels.value_counts())\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cd598-c09f-4d32-8f4d-0fc6f1461979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are combining a test_category_removed_groupedby_prov provider column and fraud or not prediction that we found above that is labels.\n",
    "#And then save it as a submission log file.The file contains provider id and they are fraud or not\n",
    "submission_log = pd.DataFrame({\"Provider\":test_category_removed_groupedby_prov.Provider})\n",
    "submission_log[\"Potential_fraud\"] = labels\n",
    "print(submission_log.shape)\n",
    "submission_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13108df-1c1a-4e8c-9abf-ba30b7fd34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission file\n",
    "submission_log.to_csv(\"Submission_logistic_regresssion_threshold_60.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11f60e-4c75-4bd7-9841-2c0343dabe6a",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85186f-f10f-43c4-a127-1e30d3c9ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try random forest to do the same\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 500,class_weight = \"balanced\",random_state = 42,max_depth = 4)\n",
    "rfc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e163e0-0203-4487-bdb9-c34a73a87888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#roc curve \n",
    "x_val_pred_prob = rfc.predict_proba(x_val)\n",
    "fpr,tpr,threshold = roc_curve(y_val,x_val_pred_prob[:,1])\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "plt.plot(fpr,tpr,lw = 1,label = f\"roc curve (AUC = {roc_auc:.2f}%)\")\n",
    "plt.plot([0,1],[0,1],ls = \"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.01])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297483d1-0bd7-465a-962f-059301782790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution plot for both fpr and tpr\n",
    "sns.distplot(tpr,color = \"darkblue\")\n",
    "sns.distplot(fpr,color = \"red\")\n",
    "plt.text(0.1,5,\"Negatives\",color = \"red\")\n",
    "plt.text(0.8,5,\"Positives\",color = \"darkblue\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.xlim([-0.2,1.2])\n",
    "plt.ylabel(\"Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba2757-50a0-4c6a-a895-d1d26cd1357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by default threshold is 0.5 so we don't need to choose a thresholt\n",
    "rfc_train_pred = rfc.predict(x_train)\n",
    "rfc_val_pred = rfc.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0db9fe-1524-4a7d-85a2-0a29c6f5dc59",
   "metadata": {},
   "source": [
    "***RandomForest : Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b97512-efb9-44b7-9ee7-aeeec88ee308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "rfc_cm_train = confusion_matrix(y_train,rfc_train_pred,labels = [1,0])\n",
    "rfc_cm_val = confusion_matrix(y_val,rfc_val_pred,labels = [1,0])\n",
    "print(\"Random forest train data prediction confusion matrix:\\n\",rfc_cm_train)\n",
    "print(\"Random forest val data prediction confusion matrix:\\n\",rfc_cm_val)\n",
    "\n",
    "print(\"Random forest train data prediction accuracy score:\",accuracy_score(y_train,rfc_train_pred))\n",
    "print(\"Random forest val data prediction accuracy score:\",accuracy_score(y_val,rfc_val_pred))\n",
    "\n",
    "print(\"Random forest train data prediction precision score:\",accuracy_score(y_train,rfc_train_pred))\n",
    "print(\"Random forest val data prediction precision score:\",accuracy_score(y_val,rfc_val_pred))\n",
    "\n",
    "print(\"Random forest train data prediction recall score:\",recall_score(y_train,rfc_train_pred))\n",
    "print(\"Random forest val data prediction recall score:\",recall_score(y_val,rfc_val_pred))\n",
    "\n",
    "print(\"Random forest train data prediction f1 score:\",f1_score(y_train,rfc_train_pred))\n",
    "print(\"Random forest val data prediction f1 score:\",f1_score(y_val,rfc_val_pred))\n",
    "\n",
    "print(\"Random forest train data prediction area under curve score:\",roc_auc_score(y_train,rfc_train_pred))\n",
    "print(\"Random forest val data prediction area under curve score:\",roc_auc_score(y_val,rfc_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bd4b2-8043-4438-8c19-f025bc8c54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestFeature importance\n",
    "feature_list = list(test_category_removed_groupedby_prov.columns[1:])#here the providerid column also included so we have to remove it because we removed that column in the dataset given to the model. \n",
    "print(feature_list[:5])\n",
    "importances = list(rfc.feature_importances_)#this returns feature importance scores in the order that we given the data to the model.\n",
    "print(importances[:5])\n",
    "#using zip function we merge the feature name and their importances\n",
    "feature_importances = [(feature,round(score,2))for feature,score in zip(feature_list,importances)]\n",
    "#creating a dataframe \n",
    "df_feature_importances = pd.DataFrame(feature_importances,columns = [\"Features\",\"Scores\"])\n",
    "df_feature_importances.set_index(\"Features\",inplace = True)\n",
    "#now sorting the dataframe based on the scores of the feature\n",
    "df_feature_importances = df_feature_importances.sort_values(by = \"Scores\",ascending = False)\n",
    "df_feature_importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f841967-2b81-4389-b75a-568804a3234e",
   "metadata": {},
   "source": [
    "***Saving RandomForestClassifier model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dccc6f-0e28-42aa-8d4c-a0478a5c9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Random Forest model without \n",
    "joblib.dump(rfc, 'random_forest.joblib')\n",
    "joblib.dump(sc, 'random_forest_scaler.joblib')  # Same scaler as logistic regression\n",
    "\n",
    "# Save performance metrics\n",
    "rf_metrics = {\n",
    "    'model_type': 'RandomForest',\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 4,\n",
    "    'train_accuracy': accuracy_score(y_train, rfc_train_pred),\n",
    "    'val_accuracy': accuracy_score(y_val, rfc_val_pred),\n",
    "    'train_f1': f1_score(y_train, rfc_train_pred),\n",
    "    'val_f1': f1_score(y_val, rfc_val_pred)\n",
    "}\n",
    "\n",
    "with open('random_forest_metadata.json', 'w') as f:\n",
    "    json.dump(rf_metrics, f, indent=2)\n",
    "\n",
    "# Test loading\n",
    "loaded_rf_model = joblib.load('random_forest.joblib')\n",
    "loaded_rf_scaler = joblib.load('random_forest_scaler.joblib')\n",
    "print(\"Random Forest saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc0ff1-1058-42c8-aadb-c86369d8b9b4",
   "metadata": {},
   "source": [
    "***Random Forest : Prediction of Test Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be22a0-a1f2-4b94-9cd6-e1372cc9a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction of test data\n",
    "rfc_test_prediction = rfc.predict(x_test_std)\n",
    "print(rfc_test_prediction[:5])\n",
    "#merging the predicted values with their respective providerid\n",
    "merged_prov_pf = pd.DataFrame({\n",
    "    \"Provider\":test_category_removed_groupedby_prov.Provider,\n",
    "    \"PotentialFraud\":rfc_test_prediction})\n",
    "#replace 1 and 0 with yes and no\n",
    "merged_prov_pf[\"PotentialFraud\"] = merged_prov_pf[\"PotentialFraud\"].replace({0:\"No\",1:\"Yes\"})\n",
    "merged_prov_pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf88e6-7220-4801-8aed-0202e4fb0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission\n",
    "print(merged_prov_pf.shape)\n",
    "merged_prov_pf.to_csv(\"Submission_Random_Forest_Classifier.csv\",index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db1eed-7d91-4d4a-84f9-53c88080701e",
   "metadata": {},
   "source": [
    "### Lets try some Unsupervised Learning models if we get a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980cf454-f22e-4435-a528-b03117f942ee",
   "metadata": {},
   "source": [
    "### principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102a4ed-10d6-4dfc-a5a9-6989bb5143f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category_removed_groupedby_prov.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06037ff-ae39-4b24-9920-3739b64e535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_category_removed_groupedby_prov.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382fdbd-84b8-4c7c-988e-85b9bc31db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing a  both train and val data \n",
    "std = StandardScaler()\n",
    "std.fit(train_category_removed_groupedby_prov.iloc[:,2:])#for train data we are removing provider and potentialfraud columns.\n",
    "train_category_removed_groupedby_prov_scaled = std.transform(train_category_removed_groupedby_prov.iloc[:,2:])\n",
    "test_category_removed_groupedby_prov_scaled = std.transform(test_category_removed_groupedby_prov.iloc[:,1:])#here we removed a provider column because id doesn't give any value to model.\n",
    "#converting both the scaled into dataframe\n",
    "train_category_removed_groupedby_prov_scaled = pd.DataFrame(train_category_removed_groupedby_prov_scaled)\n",
    "test_category_removed_groupedby_prov_scaled = pd.DataFrame(test_category_removed_groupedby_prov_scaled)\n",
    "print(train_category_removed_groupedby_prov_scaled.shape)\n",
    "print(test_category_removed_groupedby_prov_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149caa17-93fc-4b21-9521-7734a9de850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Maximum Variance\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 29)\n",
    "pca.fit(train_category_removed_groupedby_prov_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce4cf3-9109-4acb-9c32-3a55fbc489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca.explained_variance_ratio_ returns the variance of each fitted data.\n",
    "#np.round is just like a round()\n",
    "print(\"pca explained variance :\\n\",np.round(pca.explained_variance_ratio_,3))\n",
    "#now transform both train and test data and then the total number of datapoints decomposeto the given datapoints\n",
    "train_pca = pca.transform(train_category_removed_groupedby_prov_scaled)\n",
    "test_pca = pca.transform(test_category_removed_groupedby_prov_scaled)\n",
    "print(\"train pca shape:\",train_pca.shape)\n",
    "print(\"test pca shape:\",test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d8263-f81d-408e-bb31-ac3c23c7c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now converting both train and test data into dataframe\n",
    "train_pca = pd.DataFrame(train_pca)\n",
    "test_pca = pd.DataFrame(test_pca)\n",
    "print(\"train pca dataframe:\",train_pca.head())\n",
    "print(\"test pca dataframe:\",test_pca.head())\n",
    "#Now adding a potential fraud column that we removed before a train a data now we are adding that again into a pca.\n",
    "train_pca[\"PotentialFraud\"] = train_category_removed_groupedby_prov.PotentialFraud\n",
    "train_pca.to_csv(\"train_pca.csv\",index = False)\n",
    "test_pca.to_csv(\"test_pca.csv\",index = False)\n",
    "train_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a462d-17e6-48d5-942c-fb4578f65d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Split out features and label from your PCA DataFrame\n",
    "X_pca_train = train_pca.drop(columns=\"PotentialFraud\")\n",
    "y_pca_train = train_pca[\"PotentialFraud\"].astype(int)   # target\n",
    "\n",
    "# Fit Isolation Forest (unsupervised!)\n",
    "iso = IsolationForest(\n",
    "    n_estimators    = 100,\n",
    "    contamination   = 0.09,\n",
    "    random_state    = 42\n",
    ")\n",
    "iso.fit(X_pca_train)\n",
    "\n",
    "# Predict on TRAIN to see how well it isolates known fraud\n",
    "# -1 â†’ anomaly (fraud), +1 â†’ normal\n",
    "raw_train_pred = iso.predict(X_pca_train)\n",
    "y_train_if = (raw_train_pred == -1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50898c16-a359-408e-bbeb-02fcc8f7d09e",
   "metadata": {},
   "source": [
    "***Isolation Forest : Model Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416308b-7162-4b1d-a531-182a2af0b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Train Classification Report ===\\n\")\n",
    "print(classification_report(y_pca_train, y_train_if,\n",
    "      target_names=[\"Normal\",\"Fraud\"]))\n",
    "\n",
    "cm = confusion_matrix(y_pca_train, y_train_if, labels=[0,1])\n",
    "print(\"=== Train Confusion Matrix ===\")\n",
    "print(pd.DataFrame(cm,\n",
    "                   index=[\"TrueNormal\",\"TrueFraud\"],\n",
    "                   columns=[\"PredNormal\",\"PredFraud\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b1869-e34e-48d2-a432-d43777ff6e02",
   "metadata": {},
   "source": [
    "***Saving a IsolationForest Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecfa50-6bae-4c0a-b944-34b1eb1049a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Isolation Forest model using joblib\n",
    "joblib.dump(iso, 'isolation_forest.joblib')\n",
    "joblib.dump(std, 'isolation_forest_pca_scaler.joblib')  # PCA scaler\n",
    "joblib.dump(pca, 'isolation_forest_pca_transformer.joblib')  # PCA transformer\n",
    "\n",
    "# Saving a metrics as a json file\n",
    "iso_metrics = {\n",
    "    'model_type': 'IsolationForest',\n",
    "    'contamination': 0.09,\n",
    "    'n_estimators': 100,\n",
    "    'pca_components': 29\n",
    "}\n",
    "\n",
    "with open('isolation_forest_metadata.json', 'w') as f:\n",
    "    json.dump(iso_metrics, f, indent=2)\n",
    "\n",
    "# loading a svaed model\n",
    "loaded_iso_model = joblib.load('isolation_forest.joblib')\n",
    "loaded_iso_scaler = joblib.load('isolation_forest_pca_scaler.joblib')\n",
    "loaded_pca = joblib.load('isolation_forest_pca_transformer.joblib')\n",
    "print(\"Isolation Forest saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7ee8d-4584-4e9e-a51c-9a303fb09fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict the test data and create a submission file\n",
    "X_pca_test = test_pca.copy()   # your 1353Ã—29 PCA DataFrame\n",
    "raw_test_pred = iso.predict(X_pca_test)\n",
    "y_test_if = (raw_test_pred == -1).astype(int)\n",
    "\n",
    "#\n",
    "submission = pd.DataFrame({\n",
    "    \"Provider\"       : test_category_removed_groupedby_prov[\"Provider\"],\n",
    "    \"PotentialFraud\" : np.where(y_test_if==1, \"Yes\", \"No\")\n",
    "})\n",
    "print(submission.shape)\n",
    "submission.to_csv(\"Submission_Isolation_Forest.csv\",index = False) \n",
    "print(\"\\nSubmission value counts:\")\n",
    "print(submission[\"PotentialFraud\"].value_counts())\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358daaa-5db1-4aba-9248-77d17c182c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
